{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abeebyekeen/DLforBeginners/blob/main/PytorchBasis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PyTorch**\n",
        "\n",
        "PyTorch is one of DL frameworks created by Facebook. PyTorch is often seen as beginner-friendly for research.\n",
        "\n",
        "# **Tensors**\n",
        "Today, we'll explore the basics of PyTorch. Tensors are a central component of PyTorch and are used for all kinds of operations in machine learning and deep learning.\n",
        "\n",
        "What is a Tensor?\n",
        "\n",
        "A tensor is a multi-dimensional array used to represent data in deep learning models. Tensors can have various dimensions:\n",
        "\n",
        "0D (a scalar) contains a single value.\n",
        "\n",
        "1D (a vector) is an array of numbers.\n",
        "\n",
        "2D (a matrix) is a two-dimensional array of numbers.\n",
        "\n",
        "Higher-dimensional tensors have more than two dimensions.\n",
        "Tensors in PyTorch are similar to NumPy arrays but with additional capabilities that are useful for deep learning."
      ],
      "metadata": {
        "id": "b3UKKa89hw4i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRpbGHXBGO-C"
      },
      "outputs": [],
      "source": [
        "### box 1.1\n",
        "import torch\n",
        "\n",
        "# Scalar\n",
        "x0 = torch.tensor(4)\n",
        "print('x0:', x0)\n",
        "# Vector\n",
        "x1 = torch.tensor([1, 2, 3])\n",
        "print('x1:', x1)\n",
        "# Matrix\n",
        "x2 = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "print('x2:', x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tensors pratice**\n",
        "Next, we'll see how to create random tensors and access specific elements, rows, and columns within them."
      ],
      "metadata": {
        "id": "kZtsnovtj7ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### box 1.2\n",
        "# Creating a random tensor of size 3x4 between 0 and 1\n",
        "random_tensor = torch.rand(3, 4)\n",
        "print('random_tensor:', random_tensor)\n",
        "\n",
        "# Accessing a specific element (e.g., [2, 3])\n",
        "element = random_tensor[1, 2]\n",
        "print('element:', element)\n",
        "\n",
        "# Accessing the first row\n",
        "first_row = random_tensor[0]\n",
        "print('first_row:', first_row)\n",
        "\n",
        "# Accessing the third column\n",
        "third_column = random_tensor[:, 2]\n",
        "print('third_column:', third_column)\n",
        "\n",
        "# Complete the code to display all elements from second column to the end.\n",
        "sub_tensor1 =\n",
        "print('Array 1:')\n",
        "\n",
        "# Complete the code to display all elements in the first two rows of an array.\n",
        "\n",
        "sub_tensor2 =\n",
        "print('Array 2:')\n"
      ],
      "metadata": {
        "id": "5QCL0HsfGdod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NumPy arrays and PyTorch tensors**\n",
        "Now, we'll learn how to convert data between NumPy arrays and PyTorch tensors. This is a valuable skill for integrating PyTorch with other Python libraries and for data preprocessing."
      ],
      "metadata": {
        "id": "NaDQw1TGkQOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### box 1.3\n",
        "import numpy as np\n",
        "\n",
        "# Convert NumPy array to PyTorch tensor\n",
        "numpy_array = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "pytorch_tensor = torch.from_numpy(numpy_array)\n",
        "#pytorch_tensor = torch.tensor(numpy_array)\n",
        "print('pytorch_tensor:', pytorch_tensor)\n",
        "\n",
        "# Convert PyTorch tensor back to NumPy array\n",
        "converted_array = pytorch_tensor.numpy()\n",
        "\n",
        "print('converted_array:', converted_array)\n"
      ],
      "metadata": {
        "id": "aM75-GU4G45U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Autograd**\n",
        "Autograd is a core PyTorch package that provides automatic differentiation capabilities. Autograd is essential for computing backward passes in neural networks and for optimizing gradients in various training algorithms. Compare to numpy, pytorch simplifies the gradient computation process, making it easier to develop and train neural network models."
      ],
      "metadata": {
        "id": "s_Dt7cJAMWim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### box 1.4\n",
        "import torch\n",
        "\n",
        "# Create a tensor and set requires_grad=True to track computation\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "y = x ** 2\n",
        "print(y)\n",
        "\n",
        "\n",
        "y.backward()  # Computes the gradient dy/dx = 2x\n",
        "print(x.grad)  # Outputs gradient at x=2 -> tensor([4.0])\n",
        "\n",
        "\n",
        "#%% compute gradient for a tensor array x\n",
        "# x = torch.tensor(torch.arange(-2,2,0.01), requires_grad=True)\n",
        "# y = x ** 2\n",
        "# y.backward(torch.ones_like(x))  # Computes the gradient dy/dx for each x_i\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(x.detach().numpy(),y.detach().numpy(), color = 'r')\n",
        "# plt.plot(x.detach().numpy(),x.grad.detach().numpy(), color = 'b')\n"
      ],
      "metadata": {
        "id": "_OZfKbrGMaYS",
        "outputId": "11890b47-bb2b-4c85-dbe2-2f3ecc2ed211",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4.], grad_fn=<PowBackward0>)\n",
            "tensor([4.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Detach Tensor** #\n",
        "Detach tensors from the computation graph and perform operations without gradient tracking in PyTorch. Detaching a tensor is useful when you want to perform operations on it but don't need to compute gradients."
      ],
      "metadata": {
        "id": "ujYHgSrel1aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### box 1.5\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "y = x ** 2\n",
        "\n",
        "detached_y = y.detach()\n",
        "\n",
        "with torch.no_grad():\n",
        "  z = y * 2\n",
        "\n",
        "print(detached_y)\n",
        "print(z)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w3DsAQSYNTXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradient Management in PyTorch**\n",
        "\n",
        "We will highlight how gradients accumulate and the importance of resetting them in each iteration.\n",
        "\n",
        "**Why Reset the Gradients?**\n",
        "\n",
        "In PyTorch, gradients accumulate by default. This means that each time backward() is called, gradients are added to the existing grad attribute of the tensor, rather than replacing them."
      ],
      "metadata": {
        "id": "Ktbwh-Y2wYQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### box 1.6\n",
        "x = torch.ones(3, requires_grad=True)\n",
        "for epoch in range(3):\n",
        "  y = x.sum()\n",
        "  y.backward()\n",
        "  print(x.grad)\n",
        "\n",
        "  # Reset the gradient.\n",
        "  # x.grad.zero_()"
      ],
      "metadata": {
        "id": "tPDW4uUuPX6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic Training Loop**\n",
        "\n",
        "In this exercise, we will demonstrate a simple training loop using PyTorch,"
      ],
      "metadata": {
        "id": "FZIYVvOom_fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### box 1.7\n",
        "## Step 0: import package\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "## Step 1: Generating synthetic data\n",
        "x_train = torch.randn(100, 1)  # 100 samples, 1 feature\n",
        "y_train = 4*x_train + 1  # A simple linear relationship with some noise\n",
        "\n",
        "## Step 2: Create a Model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1)  # A simple linear layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "## Step 3: Define Loss Function and Optimizer\n",
        "model = SimpleModel()\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
        "\n",
        "## Step 4: Training loop\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    # Forward pass\n",
        "    predictions = model(x_train)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(predictions, y_train)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()  # IMPORTANT: Reset gradients to zero before backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update model parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "j_39NCoYwBPf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}